<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Nixos on marcus</title><link>https://marcus.means.no/categories/nixos/</link><description>Recent content in Nixos on marcus</description><generator>Hugo</generator><language>en-us</language><copyright>Code released under the MIT license.</copyright><lastBuildDate>Tue, 02 Apr 2024 04:27:33 +0000</lastBuildDate><atom:link href="https://marcus.means.no/categories/nixos/index.xml" rel="self" type="application/rss+xml"/><item><title>Easily Run LLMs on nixOS</title><link>https://marcus.means.no/post/2024-01-02-easily-run-llms-on-nixos/</link><pubDate>Tue, 02 Jan 2024 00:00:00 +0000</pubDate><guid>https://marcus.means.no/post/2024-01-02-easily-run-llms-on-nixos/</guid><description>&lt;p>With the recent AI craze, it&amp;rsquo;s fun to play with local models. I like &lt;a href="https://ollama.ai/">ollama&lt;/a>,
which lets you easily download and interact with various models. Thanks to this
recently merged &lt;a href="https://github.com/NixOS/nixpkgs/pull/277442">PR&lt;/a>, it&amp;rsquo;s now trivial to run your own ollama
service on nixos:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-nix" data-lang="nix">&lt;span style="display:flex;">&lt;span>services&lt;span style="color:#f92672">.&lt;/span>ollama&lt;span style="color:#f92672">.&lt;/span>enable &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#66d9ef">true&lt;/span>;
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Adding this to your host config and rebuilding is all it takes to start the service,
and you can then interact with it via the cli or rest interface. For instance to talk
to the &lt;a href="https://mistral.ai/product/">mistral&lt;/a> model, you can do:&lt;/p></description></item></channel></rss>